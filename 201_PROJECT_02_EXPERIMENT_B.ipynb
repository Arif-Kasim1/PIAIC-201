{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSgWg8ZCSRX/KgBSmGwN+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arif-Kasim1/PIAIC-201/blob/main/201_PROJECT_02_EXPERIMENT_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lQuGETQLckA",
        "outputId": "96cf2871-4257-4e5a-e514-accfd8ea9a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/798.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.9/199.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.3.3 requires langchain-core<0.4.0,>=0.3.25, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq langchain==0.1.0 langchain-google-genai==0.0.6 pinecone-client==3.0.0 google-generativeai==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq pinecone-client==3.0.0 langchain langchain-openai openai"
      ],
      "metadata": {
        "id": "syV_njLptTht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pinecone\n",
        "import os\n",
        "import textwrap\n",
        "from typing import List, Dict\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "class PineconeRAG:\n",
        "    def __init__(self, api_key: str, environment: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system with Pinecone\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Pinecone API key\n",
        "            environment (str): Pinecone environment\n",
        "            index_name (str): Name of the Pinecone index\n",
        "        \"\"\"\n",
        "        # Set API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        self.pinecone_api_key = api_key\n",
        "        self.environment = environment\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize embeddings and LLM\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "        # Initialize Pinecone with the new client\n",
        "        pc = pinecone.Pinecone(api_key=self.pinecone_api_key)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in pc.list_indexes().names():\n",
        "            pc.create_index(\n",
        "                name=self.index_name,\n",
        "                spec=pinecone.Spec(\n",
        "                    dimension=768,  # Dimension for Google's embedding model\n",
        "                    metric=\"cosine\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Connect to Pinecone\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def process_documents(self, file_path: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Process documents and split into chunks\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the text file\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of text chunks\n",
        "        \"\"\"\n",
        "        # Read the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            raw_text = file.read()\n",
        "\n",
        "        # Create text splitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\nTitle:\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Split text into chunks\n",
        "        return text_splitter.split_text(raw_text)\n",
        "\n",
        "    def upload_to_pinecone(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Upload text chunks to Pinecone\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of text chunks to upload\n",
        "        \"\"\"\n",
        "        # Create vector store from texts\n",
        "        Pinecone.from_texts(\n",
        "            texts,\n",
        "            self.embeddings,\n",
        "            index_name=self.index_name\n",
        "        )\n",
        "        print(f\"Uploaded {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def create_qa_chain(self) -> RetrievalQA:\n",
        "        \"\"\"\n",
        "        Create the question-answering chain\n",
        "\n",
        "        Returns:\n",
        "            RetrievalQA: The QA chain\n",
        "        \"\"\"\n",
        "        # Create prompt template\n",
        "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        # Create chain\n",
        "        chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.vector_store.as_retriever(\n",
        "                search_kwargs={\"k\": 3}\n",
        "            ),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT}\n",
        "        )\n",
        "\n",
        "        return chain\n",
        "\n",
        "    def ask_question(self, question: str, chain: RetrievalQA) -> Dict:\n",
        "        \"\"\"\n",
        "        Ask a question and get response\n",
        "\n",
        "        Args:\n",
        "            question (str): Question to ask\n",
        "            chain (RetrievalQA): The QA chain\n",
        "\n",
        "        Returns:\n",
        "            Dict: Response including answer and source documents\n",
        "        \"\"\"\n",
        "        # Get response\n",
        "        start_time = time.time()\n",
        "        response = chain(question)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Format response\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"answer\": response['result'],\n",
        "            \"sources\": [doc.page_content for doc in response['source_documents']],\n",
        "            \"time_taken\": f\"{end_time - start_time:.2f} seconds\"\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def delete_index(self) -> None:\n",
        "        \"\"\"Delete the Pinecone index\"\"\"\n",
        "        pc = pinecone.Pinecone(api_key=self.pinecone_api_key)\n",
        "        if self.index_name in pc.list_indexes().names():\n",
        "            pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "def main():\n",
        "    # Initialize RAG system\n",
        "    rag = PineconeRAG(\n",
        "        api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        environment=\"gcp-starter\",  # Make sure to use your actual environment\n",
        "        index_name=\"tech-articles\"\n",
        "    )\n",
        "\n",
        "    # Process and upload documents\n",
        "    texts = rag.process_documents('/content/Data.txt')\n",
        "    rag.upload_to_pinecone(texts)\n",
        "\n",
        "    # Create QA chain\n",
        "    qa_chain = rag.create_qa_chain()\n",
        "\n",
        "    # Example questions\n",
        "    questions = [\n",
        "        \"What are the main applications of blockchain?\",\n",
        "        \"How does quantum computing differ from classical computing?\",\n",
        "        \"What are the advantages of 5G networks?\"\n",
        "    ]\n",
        "\n",
        "    # Ask questions\n",
        "    for question in questions:\n",
        "        result = rag.ask_question(question, qa_chain)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"\\nQuestion: {result['question']}\")\n",
        "        print(f\"\\nAnswer: {textwrap.fill(result['answer'], width=80)}\")\n",
        "        print(f\"\\nTime taken: {result['time_taken']}\")\n",
        "        print(\"\\nSources used:\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\nSource {i}:\")\n",
        "            print(textwrap.fill(source, width=80))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "1dFzgXlWLeEA",
        "outputId": "9433bd62-6432-4779-851c-9ed04c5c7d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pinecone' has no attribute 'Spec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d440bacfca52>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d440bacfca52>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Initialize RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     rag = PineconeRAG(\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gcp-starter\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Make sure to use your actual environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d440bacfca52>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, environment, index_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m             pc.create_index(\n\u001b[1;32m     39\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 spec=pinecone.Spec(\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Dimension for Google's embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pinecone' has no attribute 'Spec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pinecone\n",
        "import os\n",
        "import textwrap\n",
        "from typing import List, Dict\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "class PineconeRAG:\n",
        "    def __init__(self, api_key: str, environment: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system with Pinecone\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Pinecone API key\n",
        "            environment (str): Pinecone environment\n",
        "            index_name (str): Name of the Pinecone index\n",
        "        \"\"\"\n",
        "        # Set API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        self.pinecone_api_key = api_key\n",
        "        self.environment = environment\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize embeddings and LLM\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "        # Initialize Pinecone\n",
        "        # pinecone.init(api_key=self.pinecone_api_key, environment=self.environment)\n",
        "        pc = pinecone.Pinecone(api_key=self.pinecone_api_key)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in pinecone.list_indexes():\n",
        "            pinecone.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=768,  # Dimension for Google's embedding model\n",
        "                metric='cosine'\n",
        "            )\n",
        "\n",
        "        # Connect to Pinecone\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def process_documents(self, file_path: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Process documents and split into chunks\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the text file\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of text chunks\n",
        "        \"\"\"\n",
        "        # Read the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            raw_text = file.read()\n",
        "\n",
        "        # Create text splitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\nTitle:\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Split text into chunks\n",
        "        return text_splitter.split_text(raw_text)\n",
        "\n",
        "    def upload_to_pinecone(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Upload text chunks to Pinecone\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of text chunks to upload\n",
        "        \"\"\"\n",
        "        # Create vector store from texts\n",
        "        Pinecone.from_texts(\n",
        "            texts,\n",
        "            self.embeddings,\n",
        "            index_name=self.index_name\n",
        "        )\n",
        "        print(f\"Uploaded {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def create_qa_chain(self) -> RetrievalQA:\n",
        "        \"\"\"\n",
        "        Create the question-answering chain\n",
        "\n",
        "        Returns:\n",
        "            RetrievalQA: The QA chain\n",
        "        \"\"\"\n",
        "        # Create prompt template\n",
        "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        # Create chain\n",
        "        chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.vector_store.as_retriever(\n",
        "                search_kwargs={\"k\": 3}\n",
        "            ),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT}\n",
        "        )\n",
        "\n",
        "        return chain\n",
        "\n",
        "    def ask_question(self, question: str, chain: RetrievalQA) -> Dict:\n",
        "        \"\"\"\n",
        "        Ask a question and get response\n",
        "\n",
        "        Args:\n",
        "            question (str): Question to ask\n",
        "            chain (RetrievalQA): The QA chain\n",
        "\n",
        "        Returns:\n",
        "            Dict: Response including answer and source documents\n",
        "        \"\"\"\n",
        "        # Get response\n",
        "        start_time = time.time()\n",
        "        response = chain(question)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Format response\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"answer\": response['result'],\n",
        "            \"sources\": [doc.page_content for doc in response['source_documents']],\n",
        "            \"time_taken\": f\"{end_time - start_time:.2f} seconds\"\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def delete_index(self) -> None:\n",
        "        \"\"\"Delete the Pinecone index\"\"\"\n",
        "        if self.index_name in pinecone.list_indexes():\n",
        "            pinecone.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "def main():\n",
        "    # Initialize RAG system\n",
        "    rag = PineconeRAG(\n",
        "        api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        environment=\"gcp-starter\",  # Make sure to use your actual environment\n",
        "        index_name=\"tech-articles\"\n",
        "    )\n",
        "\n",
        "    # Process and upload documents\n",
        "    texts = rag.process_documents('/content/Data.txt')\n",
        "    rag.upload_to_pinecone(texts)\n",
        "\n",
        "    # Create QA chain\n",
        "    qa_chain = rag.create_qa_chain()\n",
        "\n",
        "    # Example questions\n",
        "    questions = [\n",
        "        \"What are the main applications of blockchain?\",\n",
        "        \"How does quantum computing differ from classical computing?\",\n",
        "        \"What are the advantages of 5G networks?\"\n",
        "    ]\n",
        "\n",
        "    # Ask questions\n",
        "    for question in questions:\n",
        "        result = rag.ask_question(question, qa_chain)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"\\nQuestion: {result['question']}\")\n",
        "        print(f\"\\nAnswer: {textwrap.fill(result['answer'], width=80)}\")\n",
        "        print(f\"\\nTime taken: {result['time_taken']}\")\n",
        "        print(\"\\nSources used:\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\nSource {i}:\")\n",
        "            print(textwrap.fill(source, width=80))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YMjMKtAKtp9M",
        "outputId": "c562fb59-1017-471a-f898-7a46bc6bfdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pinecone' has no attribute 'list_indexes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-59803d8cb95a>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-59803d8cb95a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Initialize RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     rag = PineconeRAG(\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gcp-starter\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Make sure to use your actual environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-59803d8cb95a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, environment, index_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Create index if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpinecone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             pinecone.create_index(\n\u001b[1;32m     40\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pinecone' has no attribute 'list_indexes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, openai_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize OpenAI components\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                spec=pinecone.Spec(\n",
        "                    dimension=1536,  # OpenAI embedding dimension\n",
        "                    metric=\"cosine\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=\"your-pinecone-api-key\",\n",
        "        openai_api_key=\"your-openai-api-key\",\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ftq_jLkoueEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, google_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize Gemini components\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                spec=pinecone.Spec(\n",
        "                    dimension=768,  # Gemini embedding dimension\n",
        "                    metric=\"cosine\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "fVGLmpdvvudJ",
        "outputId": "88d0a04b-a897-4fd7-96ab-8e863a03205e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pinecone' has no attribute 'Spec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-47f1479e6a8c>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-47f1479e6a8c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Initialize the RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     rag = SimpleRAG(\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mpinecone_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-47f1479e6a8c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pinecone_api_key, google_api_key, index_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m             self.pc.create_index(\n\u001b[1;32m     30\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 spec=pinecone.Spec(\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Gemini embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pinecone' has no attribute 'Spec'"
          ]
        }
      ]
    },
    {
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec # Import the ServerlessSpec\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, google_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize Gemini components\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                spec=ServerlessSpec( # Use ServerlessSpec\n",
        "                    dimension=768,  # Gemini embedding dimension\n",
        "                    metric=\"cosine\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "HwLKqfLjycpJ",
        "outputId": "fb0ab680-523a-4b1c-8ca9-b8d53b40ea29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ServerlessSpec.__new__() got an unexpected keyword argument 'dimension'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-209f2cbf7aaf>\u001b[0m in \u001b[0;36m<cell line: 120>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-209f2cbf7aaf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Initialize the RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     rag = SimpleRAG(\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mpinecone_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-209f2cbf7aaf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pinecone_api_key, google_api_key, index_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m             self.pc.create_index(\n\u001b[1;32m     31\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 spec=ServerlessSpec( # Use ServerlessSpec\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Gemini embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ServerlessSpec.__new__() got an unexpected keyword argument 'dimension'"
          ]
        }
      ]
    },
    {
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec # Import the ServerlessSpec\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, google_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize Gemini components\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=768,  # Gemini embedding dimension\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec( # Use ServerlessSpec\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-west-2\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "zwfIPMmryooA",
        "outputId": "f7a89e6b-149a-4934-b9c7-52165467c6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PineconeApiException",
          "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-04', 'X-Cloud-Trace-Context': '4a343347a885ddee6fb0145bd8f21ff1', 'Date': 'Thu, 26 Dec 2024 11:47:02 GMT', 'Server': 'Google Frontend', 'Content-Length': '200', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"INVALID_ARGUMENT\",\"message\":\"Bad request: Your free plan does not support indexes in the us-west-2 region of aws. To create indexes in this region, upgrade your plan.\"},\"status\":400}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-95ea7a3f66cc>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-95ea7a3f66cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Initialize the RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     rag = SimpleRAG(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mpinecone_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-95ea7a3f66cc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pinecone_api_key, google_api_key, index_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Create index if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             self.pc.create_index(\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Gemini embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/control/pinecone.py\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(self, name, dimension, spec, metric, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mapi_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_index_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCreateIndexRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServerlessSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mapi_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_index_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCreateIndexRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPodSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mapi_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_index_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCreateIndexRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \"\"\"\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api/manage_indexes_api.py\u001b[0m in \u001b[0;36m__create_index\u001b[0;34m(self, create_index_request, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_index_request'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mcreate_index_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         self.create_index = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoint_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'http_method'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \"\"\"\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             return self.__call_api(resource_path, method,\n\u001b[0m\u001b[1;32m    409\u001b[0m                                    \u001b[0mpath_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                                    \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPineconeApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    452\u001b[0m                                             body=body)\n\u001b[1;32m    453\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             return self.rest_client.POST(url,\n\u001b[0m\u001b[1;32m    455\u001b[0m                                          \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                                          \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    299\u001b[0m     def POST(self, url, headers=None, query_params=None, post_params=None,\n\u001b[1;32m    300\u001b[0m              body=None, _preload_content=True, _request_timeout=None):\n\u001b[0;32m--> 301\u001b[0;31m         return self.request(\"POST\", url,\n\u001b[0m\u001b[1;32m    302\u001b[0m                             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                             \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPineconeApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2024-04', 'X-Cloud-Trace-Context': '4a343347a885ddee6fb0145bd8f21ff1', 'Date': 'Thu, 26 Dec 2024 11:47:02 GMT', 'Server': 'Google Frontend', 'Content-Length': '200', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"INVALID_ARGUMENT\",\"message\":\"Bad request: Your free plan does not support indexes in the us-west-2 region of aws. To create indexes in this region, upgrade your plan.\"},\"status\":400}\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec # Import the ServerlessSpec\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, google_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize Gemini components\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=768,  # Gemini embedding dimension\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec( # Use ServerlessSpec\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\" # Changed region to us-east-1\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4D838I-Hy9AH",
        "outputId": "1ede52dd-e47a-4710-c2b6-908c45614806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 1 chunks to Pinecone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  addendum : str, optional\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "SystemMessages are not yet supported!\n\nTo automatically convert the leading SystemMessage to a HumanMessage,\nset  `convert_system_message_to_human` to True. Example:\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9407413923da>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-9407413923da>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Ask a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is Machine Learning and how does it relate to AI?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-9407413923da>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         return {\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0m_alternative_import\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternative_import\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0m_pending\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0m_addendum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0m_package\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> T:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0monly\u001b[0m \u001b[0mone\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mShould\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mall\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0monly\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mchain\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'This is my query'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0m_run_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_noop_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0m_alternative_import\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternative_import\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0m_pending\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0m_addendum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0m_package\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> T:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mexternal_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexternal_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0m_alternative_import\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternative_import\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0m_pending\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0m_addendum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0m_package\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> T:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0monly\u001b[0m \u001b[0mone\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mShould\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mall\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0monly\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mchain\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0m_run_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_noop_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         output, extra_return_dict = self.combine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     def combine_docs(\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     ) -> Tuple[str, dict]:\n\u001b[1;32m    246\u001b[0m         \"\"\"Stuff all documents into one prompt and pass to LLM.\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_final_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     async def _acall(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0m_alternative_import\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternative_import\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0m_pending\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0m_addendum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0m_package\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> T:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0monly\u001b[0m \u001b[0mone\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mShould\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mall\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0monly\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mchain\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0minput_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \"\"\"Will be whatever keys the prompt expects.\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_final_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mls_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls_temperature\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;31m# max_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"max_tokens\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                     )\n\u001b[1;32m    407\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                         \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUUID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     ) -> LLMResult:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     ) -> ChatResult:\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         response: genai.types.GenerateContentResponse = _chat_with_retry(\n\u001b[1;32m    565\u001b[0m             \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_prepare_chat\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m     ) -> Tuple[Dict[str, Any], genai.ChatSession, genai.types.ContentDict]:\n\u001b[1;32m    645\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         history = _parse_chat_history(\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mconvert_system_message_to_human\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_system_message_to_human\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_parse_chat_history\u001b[0;34m(input_messages, convert_system_message_to_human)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconvert_system_message_to_human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         ):\n\u001b[0;32m--> 301\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \"\"\"SystemMessages are not yet supported!\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: SystemMessages are not yet supported!\n\nTo automatically convert the leading SystemMessage to a HumanMessage,\nset  `convert_system_message_to_human` to True. Example:\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec # Import the ServerlessSpec\n",
        "import textwrap\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, pinecone_api_key: str, google_api_key: str, index_name: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system\n",
        "        \"\"\"\n",
        "        # Set up API keys\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "        # Initialize Pinecone client (v3)\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Initialize Gemini components\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        # Add the convert_system_message_to_human parameter here\n",
        "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0, convert_system_message_to_human=True)\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=768,  # Gemini embedding dimension\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec( # Use ServerlessSpec\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\" # Changed region to us-east-1\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = Pinecone.from_existing_index(\n",
        "            index_name=self.index_name,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "    def load_documents(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def add_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Add texts to Pinecone\n",
        "        \"\"\"\n",
        "        self.vector_store.add_texts(texts)\n",
        "        print(f\"Added {len(texts)} chunks to Pinecone\")\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Query the RAG system\n",
        "        \"\"\"\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        response = qa_chain({\"query\": question})\n",
        "        return {\n",
        "            \"answer\": response[\"result\"],\n",
        "            \"sources\": [doc.page_content for doc in response[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"\n",
        "        Delete the Pinecone index\n",
        "        \"\"\"\n",
        "        if self.index_name in self.pc.list_indexes().names():\n",
        "            self.pc.delete_index(self.index_name)\n",
        "            print(f\"Deleted index: {self.index_name}\")\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize the RAG system\n",
        "    rag = SimpleRAG(\n",
        "        pinecone_api_key=userdata.get(\"PINECONE_API_KEY\"),\n",
        "        google_api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "        index_name=\"test-index\"\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    sample_text = \"\"\"\n",
        "    Artificial Intelligence (AI) is revolutionizing various industries.\n",
        "    Machine Learning, a subset of AI, enables systems to learn from data.\n",
        "    Deep Learning, a type of Machine Learning, uses neural networks with multiple layers.\n",
        "    Natural Language Processing (NLP) allows computers to understand human language.\n",
        "    Computer Vision helps machines interpret and analyze visual information.\n",
        "\n",
        "    Introduction: The Pentium processor series, launched by Intel in 1993,\n",
        "    marked a significant leap in performance over its predecessor, the 486 series.\n",
        "    Architecture: Introduced advanced features like superscalar architecture,\n",
        "    allowing multiple instructions per clock cycle.\n",
        "    Variants: Evolved through multiple generations, including Pentium Pro,\n",
        "    Pentium II, Pentium III, and Pentium 4, offering enhanced speed and functionality.\n",
        "    Technology: Incorporated technologies like MMX, Hyper-Threading, and higher\n",
        "    clock speeds to cater to evolving computing needs.\n",
        "    Legacy: Paved the way for modern processors, blending performance and\n",
        "    efficiency for desktops and laptops.\n",
        "\n",
        "    The future of machine learning (ML) is poised for transformative growth,\n",
        "    driving innovation across industries. Advances in neural networks, quantum\n",
        "    computing, and explainable AI will make ML more powerful and transparent.\n",
        "    It will revolutionize healthcare with precise diagnostics, enhance automation\n",
        "    in manufacturing, and optimize personalized experiences in retail and\n",
        "    entertainment. Ethical AI and robust frameworks will address challenges like\n",
        "    bias and privacy. Seamless integration with IoT, robotics, and edge\n",
        "    computing will bring AI closer to users. As ML democratizes through\n",
        "    accessible tools, its potential to solve global challenges, from climate\n",
        "    change to education, will shape a smarter, sustainable world.\n",
        "\n",
        "    Pakistanis hold a special affection for their traditional dishes, with\n",
        "    biryani, paye, and nihari reigning supreme. Biryani, a fragrant mix of rice,\n",
        "    meat, and spices, is a celebratory dish enjoyed at weddings, festivals, and\n",
        "    casual gatherings. Its rich flavors and endless variations make it a\n",
        "    nationwide favorite. Paye, a slow-cooked delicacy made from trotters,\n",
        "    offers a hearty, flavorful experience often relished during breakfast or\n",
        "    family feasts. Nihari, a spicy stew of tender meat simmered overnight, is\n",
        "    synonymous with comfort food, particularly loved in winters. These dishes\n",
        "    are more than just meals—they represent Pakistan’s rich culinary heritage\n",
        "    and the warmth of sharing food. Served with naan, parathas, or raita, they\n",
        "    bring families and friends together, embodying a deep-rooted tradition of\n",
        "    hospitality. Whether in bustling cities or quiet villages, the love for\n",
        "    biryani, paye, and nihari reflects the soul of Pakistani cuisine.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process and add documents\n",
        "    chunks = rag.load_documents(sample_text)\n",
        "    rag.add_texts(chunks)\n",
        "\n",
        "    # Ask a question\n",
        "    # question = \"What is Machine Learning and how does it relate to AI?\"\n",
        "    # question = \"What is Machine Learning future and how does it relate to AI?\"\n",
        "    # question = \"What is JSP and Servlet?\"\n",
        "    # question = \"Difference between Pentium 2 and Pentium 4?\"\n",
        "    # question = \"Difference between 486 and Pentium?\"\n",
        "    question = \"Name some traditional dishes of pakistan?\"\n",
        "\n",
        "    result = rag.query(question)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(\"\\nAnswer:\", textwrap.fill(result[\"answer\"], width=80))\n",
        "    # textwrap.fill(response, width=80)\n",
        "    print(\"\\nSources used:\")\n",
        "    for i, source in enumerate(result[\"sources\"], 1):\n",
        "        print(f\"\\nSource {i}:\", source)\n",
        "\n",
        "    # Optional: Clean up\n",
        "    # rag.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmI22qgqzce4",
        "outputId": "7d2f2096-5163-48aa-de6c-5c998b7299e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 5 chunks to Pinecone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Name some traditional dishes of pakistan?\n",
            "\n",
            "Answer: - Biryani - Paye - Nihari\n",
            "\n",
            "Sources used:\n",
            "\n",
            "Source 1: hospitality. Whether in bustling cities or quiet villages, the love for \n",
            "    biryani, paye, and nihari reflects the soul of Pakistani cuisine.\n",
            "\n",
            "Source 2: hospitality. Whether in bustling cities or quiet villages, the love for \n",
            "    biryani, paye, and nihari reflects the soul of Pakistani cuisine.\n",
            "\n",
            "Source 3: hospitality. Whether in bustling cities or quiet villages, the love for \n",
            "    biryani, paye, and nihari reflects the soul of Pakistani cuisine.\n"
          ]
        }
      ]
    }
  ]
}