{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwNCategCgIThKMo24zmYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arif-Kasim1/PIAIC-201/blob/main/201_PROJECT_02_EXPERIMENT_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk_8cMCExeeb",
        "outputId": "de7a2b0e-408b-4cac-ba5c-f9051ef97315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq langchain langchain-google-genai faiss-cpu google-generativeai typing-extensions langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "import textwrap\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\") # Set the environment variable to the correct API key\n",
        "\n",
        "# Initialize the embeddings and LLM\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
        "                          google_api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "\n",
        "def read_articles(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "def create_rag_system():\n",
        "    # Read and process the sample data\n",
        "    raw_text = read_articles('/content/Data.txt')\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\nTitle:\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "    # Create vector store\n",
        "    # FAISS (Facebook AI Similarity Search) is a library for efficient\n",
        "    # similarity search of dense vectors. Why FAISS? Fast similarity search\n",
        "    # Efficient memory usage, Good for large datasets, Optimized for vector operations\n",
        "    vector_store = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "    # Create the RAG chain with chain type=\"stuff\"\n",
        "    # Stuff: Fastest but limited by context window\n",
        "    # Map Reduce: Good for parallel processing of many documents\n",
        "    # Refine: Most detailed but can be slower\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "def create_general_chain():\n",
        "    # Create a general purpose chain for non-RAG queries\n",
        "    template = \"\"\"Question: {question}\n",
        "\n",
        "    Please provide a detailed answer.\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=template,\n",
        "    )\n",
        "\n",
        "    general_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    return general_chain\n",
        "\n",
        "def ask_question(qa_chain, general_chain, question, use_rag=True):\n",
        "    print(\"\\nQuestion:\", question)\n",
        "\n",
        "    if use_rag:\n",
        "        # Use RAG for document-specific questions\n",
        "        response = qa_chain(question)\n",
        "        print(\"\\nAnswer (Using RAG):\", textwrap.fill(response['result'], width=80))\n",
        "        print(\"\\nRelevant source chunks:\")\n",
        "        for i, doc in enumerate(response['source_documents'], 1):\n",
        "            print(f\"\\nChunk {i}:\")\n",
        "            print(textwrap.fill(doc.page_content, width=80))\n",
        "    else:\n",
        "        # Use general LLM chain for non-document questions\n",
        "        response = general_chain.run(question)\n",
        "        print(\"\\nAnswer (Using General LLM):\", textwrap.fill(response, width=80))\n",
        "\n",
        "def main():\n",
        "    # Initialize both RAG and general systems\n",
        "    rag_chain = create_rag_system()\n",
        "    general_chain = create_general_chain()\n",
        "\n",
        "    # Example questions - mix of RAG and non-RAG queries\n",
        "    questions = [\n",
        "        # RAG questions (about our documents)\n",
        "        {\"text\": \"What are the main advantages of cloud computing?\", \"use_rag\": True},\n",
        "        {\"text\": \"How do neural networks learn to recognize images?\", \"use_rag\": True},\n",
        "\n",
        "        # Non-RAG questions (general knowledge)\n",
        "        {\"text\": \"Give me the coordinates of Eiffel Tower?\", \"use_rag\": False},\n",
        "        {\"text\": \"A south asian guy is asking about rooh afza, what is that? in 50 words explanation\", \"use_rag\": False}\n",
        "    ]\n",
        "\n",
        "    # Ask each question\n",
        "    for q in questions:\n",
        "        ask_question(rag_chain, general_chain, q[\"text\"], q[\"use_rag\"])\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# The line if __name__ == \"__main__\": is a common Python idiom that determines\n",
        "# whether a Python script is being run directly or being imported as a module\n",
        "# into another script.\n",
        "# Here's what it means:\n",
        "# When you run a Python file directly (like python your_script.py):\n",
        "# Python sets the special variable __name__ to \"__main__\"\n",
        "# The code inside this if block will execute\n",
        "# When you import the file as a module in another script:\n",
        "# __name__ will be set to the module's name\n",
        "# The code inside this if block won't execute\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkiMpJ1LyMF3",
        "outputId": "a8cf0957-a7a4-4762-9473-4e58b7320077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3304bfdbd1dd>:64: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  general_chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-2-3304bfdbd1dd>:72: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain(question)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What are the main advantages of cloud computing?\n",
            "\n",
            "Answer (Using RAG): Based on the provided text, the main advantages of cloud computing are\n",
            "scalability, cost-effectiveness, and reduced maintenance overhead.\n",
            "\n",
            "Relevant source chunks:\n",
            "\n",
            "Chunk 1:\n",
            "Article: Cloud computing has transformed how businesses operate in the digital\n",
            "age. Instead of maintaining physical servers, companies can now rent computing\n",
            "resources on-demand. This model offers several advantages: scalability, cost-\n",
            "effectiveness, and reduced maintenance overhead. The three main service models\n",
            "are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and\n",
            "Software as a Service (SaaS). Major providers like AWS, Microsoft Azure, and\n",
            "Google Cloud Platform compete to\n",
            "\n",
            "Chunk 2:\n",
            "Azure, and Google Cloud Platform compete to offer increasingly sophisticated\n",
            "services, from basic storage to advanced AI capabilities.\n",
            "\n",
            "Chunk 3:\n",
            "Title: The Evolution of Cloud Computing\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Question: How do neural networks learn to recognize images?\n",
            "\n",
            "Answer (Using RAG): Neural networks learn to recognize images by analyzing example images that are\n",
            "labeled with what they depict (e.g., \"cat\" or \"no cat\").  The network builds its\n",
            "understanding through layers of interconnected nodes, each layer transforming\n",
            "the input data in increasingly complex ways.\n",
            "\n",
            "Relevant source chunks:\n",
            "\n",
            "Chunk 1:\n",
            "Article: Neural networks are computing systems inspired by biological neural\n",
            "networks in animal brains. These systems \"learn\" to perform tasks by analyzing\n",
            "training examples, generally without task-specific programming. For instance, in\n",
            "image recognition, they might learn to identify images containing cats by\n",
            "analyzing example images labeled as \"cat\" or \"no cat.\" The network builds its\n",
            "understanding through layers of interconnected nodes, each layer transforming\n",
            "the input data in increasingly\n",
            "\n",
            "Chunk 2:\n",
            "layer transforming the input data in increasingly complex ways. Deep learning,\n",
            "which involves neural networks with many layers, has revolutionized fields like\n",
            "computer vision and natural language processing.\n",
            "\n",
            "Chunk 3:\n",
            "Title: Understanding Neural Networks\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Question: Give me the coordinates of Eiffel Tower?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3304bfdbd1dd>:80: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = general_chain.run(question)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer (Using General LLM): The Eiffel Tower doesn't have a single coordinate, as it's a structure with a\n",
            "footprint and height.  To give coordinates, we need to specify a point.  The\n",
            "most common and logical point to use is the **center of the base**.  The\n",
            "coordinates of the center of the base of the Eiffel Tower are approximately:  *\n",
            "**48.8584° N, 2.2945° E**  These coordinates are given in latitude and longitude\n",
            "using the World Geodetic System 1984 (WGS 84) datum, which is the most commonly\n",
            "used standard.  You can use these coordinates with any mapping software or GPS\n",
            "device to locate the Eiffel Tower.  It's important to note that:  * **Slight\n",
            "variations exist:**  Different mapping services might show slightly different\n",
            "coordinates due to variations in surveying and data processing.  The difference\n",
            "will be minimal, usually within a few meters. * **This is the base:** This\n",
            "coordinate points to the ground level, at the center of the tower's base.  The\n",
            "coordinates will be different if you want a point higher up the tower.\n",
            "Therefore, while 48.8584° N, 2.2945° E is a highly accurate and widely accepted\n",
            "coordinate for the Eiffel Tower, remember it represents the center of its base.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Question: A south asian guy is asking about rooh afza, what is that? in 50 words explanation\n",
            "\n",
            "Answer (Using General LLM): Rooh Afza is a popular rose-flavored syrup originating from India.  It's\n",
            "intensely fragrant, sweet, and often used to make refreshing drinks, desserts,\n",
            "and even ice cream.  Think of it as a concentrated rose-flavored cordial, deeply\n",
            "ingrained in South Asian culture.  It's known for its vibrant red color and\n",
            "unique taste.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}